{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# bucket = sagemaker_session.default_bucket()\n",
    "# prefix = \"sagemaker/DEMO-pytorch-mnist\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {'local': {'local_code': True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "\n",
    "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 ```ml.c4.xlarge``` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)). The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `mnist.py` script above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role,\n",
    "    py_version=\"py38\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    source_dir=\"./resources\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"local_gpu\",\n",
    "    hyperparameters={\"epochs\": 1, \"batch_size\": 4, \"samples_per_epoch\": 500},\n",
    "    session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-08-02-22-11-19-132\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-ntx1p:\n",
      "    command: train\n",
      "    container_name: 1ub1bj9v1l-algo-1-ntx1p\n",
      "    deploy:\n",
      "      resources:\n",
      "        reservations:\n",
      "          devices:\n",
      "          - capabilities:\n",
      "            - gpu\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.11.0-gpu-py38\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-ntx1p\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpsce_mp_k/algo-1-ntx1p/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmpsce_mp_k/algo-1-ntx1p/output:/opt/ml/output\n",
      "    - /tmp/tmpsce_mp_k/algo-1-ntx1p/input:/opt/ml/input\n",
      "    - /tmp/tmpsce_mp_k/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "    - /tmp/tmpwy4t_5ov:/opt/ml/input/data/training\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpsce_mp_k/docker-compose.yaml up --build --abort-on-container-exit\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker pull 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.11.0-gpu-py38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.local.image:image pulled: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.11.0-gpu-py38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container 1ub1bj9v1l-algo-1-ntx1p  Creating\n",
      "Container 1ub1bj9v1l-algo-1-ntx1p  Created\n",
      "Attaching to 1ub1bj9v1l-algo-1-ntx1p\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:27,873 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:27,894 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:27,903 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:27,906 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:27,909 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:27,950 botocore.credentials INFO     Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:28,164 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "1ub1bj9v1l-algo-1-ntx1p  | /opt/conda/bin/python3.8 -m pip install -r requirements.txt\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Collecting SimpleITK\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Downloading SimpleITK-2.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 MB 40.8 MB/s eta 0:00:00\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 2)) (2022.7.1)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 2)) (1.22.2)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Installing collected packages: SimpleITK\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Successfully installed SimpleITK-2.2.1\n",
      "1ub1bj9v1l-algo-1-ntx1p  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "1ub1bj9v1l-algo-1-ntx1p  | [notice] A new release of pip is available: 23.0 -> 23.2.1\n",
      "1ub1bj9v1l-algo-1-ntx1p  | [notice] To update, run: pip install --upgrade pip\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,827 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,827 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,849 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,858 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,881 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,891 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,914 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,922 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:31,925 sagemaker-training-toolkit INFO     Invoking user script\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | Training Env:\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | {\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"additional_framework_parameters\": {},\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"channel_input_dirs\": {\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"training\": \"/opt/ml/input/data/training\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     },\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"current_host\": \"algo-1-ntx1p\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"current_instance_group\": \"homogeneousCluster\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"current_instance_group_hosts\": [],\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"current_instance_type\": \"local\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"distribution_hosts\": [\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"algo-1-ntx1p\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     ],\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"distribution_instance_groups\": [],\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"hosts\": [\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"algo-1-ntx1p\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     ],\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"hyperparameters\": {\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"epochs\": 1,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"batch_size\": 4,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"samples_per_epoch\": 500\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     },\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"input_data_config\": {\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"training\": {\n",
      "1ub1bj9v1l-algo-1-ntx1p  |             \"TrainingInputMode\": \"File\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         }\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     },\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"input_dir\": \"/opt/ml/input\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"instance_groups\": [],\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"instance_groups_dict\": {},\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"is_hetero\": false,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"is_master\": true,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"is_modelparallel_enabled\": null,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"is_smddpmprun_installed\": false,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"job_name\": \"pytorch-training-2023-08-02-22-11-19-132\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"log_level\": 20,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"master_hostname\": \"algo-1-ntx1p\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"model_dir\": \"/opt/ml/model\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"module_dir\": \"s3://sagemaker-us-west-2-049084878542/pytorch-training-2023-08-02-22-11-19-132/source/sourcedir.tar.gz\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"module_name\": \"train\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"network_interface_name\": \"eth0\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"num_cpus\": 8,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"num_gpus\": 1,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"num_neurons\": 0,\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"output_dir\": \"/opt/ml/output\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"resource_config\": {\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"current_host\": \"algo-1-ntx1p\",\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         \"hosts\": [\n",
      "1ub1bj9v1l-algo-1-ntx1p  |             \"algo-1-ntx1p\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  |         ]\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     },\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     \"user_entry_point\": \"train.py\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  | }\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | Environment variables:\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_HOSTS=[\"algo-1-ntx1p\"]\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_NETWORK_INTERFACE_NAME=eth0\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_HPS={\"batch_size\":4,\"epochs\":1,\"samples_per_epoch\":500}\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_USER_ENTRY_POINT=train.py\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_FRAMEWORK_PARAMS={}\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-ntx1p\",\"hosts\":[\"algo-1-ntx1p\"]}\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_CHANNELS=[\"training\"]\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_CURRENT_HOST=algo-1-ntx1p\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_CURRENT_INSTANCE_TYPE=local\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_INSTANCE_GROUPS=[]\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_INSTANCE_GROUPS_DICT={}\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_IS_HETERO=false\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_MODULE_NAME=train\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_LOG_LEVEL=20\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_INPUT_DIR=/opt/ml/input\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_OUTPUT_DIR=/opt/ml/output\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_NUM_CPUS=8\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_NUM_GPUS=1\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_NUM_NEURONS=0\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_MODEL_DIR=/opt/ml/model\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_MODULE_DIR=s3://sagemaker-us-west-2-049084878542/pytorch-training-2023-08-02-22-11-19-132/source/sourcedir.tar.gz\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-ntx1p\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-ntx1p\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-ntx1p\"],\"hyperparameters\":{\"batch_size\":4,\"epochs\":1,\"samples_per_epoch\":500},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-08-02-22-11-19-132\",\"log_level\":20,\"master_hostname\":\"algo-1-ntx1p\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-049084878542/pytorch-training-2023-08-02-22-11-19-132/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-ntx1p\",\"hosts\":[\"algo-1-ntx1p\"]},\"user_entry_point\":\"train.py\"}\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_USER_ARGS=[\"--batch_size\",\"4\",\"--epochs\",\"1\",\"--samples_per_epoch\",\"500\"]\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_HP_EPOCHS=1\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_HP_BATCH_SIZE=4\n",
      "1ub1bj9v1l-algo-1-ntx1p  | SM_HP_SAMPLES_PER_EPOCH=500\n",
      "1ub1bj9v1l-algo-1-ntx1p  | PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | Invoking script with the following command:\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | /opt/conda/bin/python3.8 train.py --batch_size 4 --epochs 1 --samples_per_epoch 500\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:32,957 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Get train data loader.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | [2023-08-02 18:16:38.184 algo-1-ntx1p:57 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "1ub1bj9v1l-algo-1-ntx1p  | /opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "1ub1bj9v1l-algo-1-ntx1p  | /opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "1ub1bj9v1l-algo-1-ntx1p  | [2023-08-02 18:16:38.336 algo-1-ntx1p:57 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \u0000\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Traceback (most recent call last):\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1082, in _try_get_data\n",
      "1ub1bj9v1l-algo-1-ntx1p  | data = self._data_queue.get(timeout=timeout)\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"/opt/conda/lib/python3.8/queue.py\", line 179, in get\n",
      "1ub1bj9v1l-algo-1-ntx1p  | self.not_empty.wait(remaining)\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"/opt/conda/lib/python3.8/threading.py\", line 306, in wait\n",
      "1ub1bj9v1l-algo-1-ntx1p  | gotit = waiter.acquire(True, timeout)\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     _error_if_any_worker_fails()\n",
      "1ub1bj9v1l-algo-1-ntx1p  | RuntimeError: DataLoader worker (pid 76) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | The above exception was the direct cause of the following exception:\n",
      "1ub1bj9v1l-algo-1-ntx1p  | \n",
      "1ub1bj9v1l-algo-1-ntx1p  | Traceback (most recent call last):\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"train.py\", line 218, in <module>\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     train(parser.parse_args())\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"train.py\", line 107, in train\n",
      "1ub1bj9v1l-algo-1-ntx1p  |     for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
      "1ub1bj9v1l-algo-1-ntx1p  | File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 539, in __next__\n",
      "1ub1bj9v1l-algo-1-ntx1p  | (data, worker_id) = self._next_data()\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1281, in _next_data\n",
      "1ub1bj9v1l-algo-1-ntx1p  | idx, data = self._get_data()\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1234, in _get_data\n",
      "1ub1bj9v1l-algo-1-ntx1p  | success, data = self._try_get_data()\n",
      "1ub1bj9v1l-algo-1-ntx1p  |   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1095, in _try_get_data\n",
      "1ub1bj9v1l-algo-1-ntx1p  | raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n",
      "1ub1bj9v1l-algo-1-ntx1p  | RuntimeError: DataLoader worker (pid(s) 76) exited unexpectedly\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:40,724 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:40,724 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:40,725 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:40,725 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "1ub1bj9v1l-algo-1-ntx1p  | ExitCode 1\n",
      "1ub1bj9v1l-algo-1-ntx1p  | ErrorMessage \"RuntimeError: DataLoader worker (pid 76) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  \n",
      "1ub1bj9v1l-algo-1-ntx1p  |  The above exception was the direct cause of the following exception\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  Traceback (most recent call last)\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  File \"train.py\", line 218, in <module>\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  train(parser.parse_args())\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  File \"train.py\", line 107, in train\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  for batch_idx, (data, target) in enumerate(train_loader, 1)\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 539, in __next__\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  (data, worker_id) = self._next_data()\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1281, in _next_data\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  idx, data = self._get_data()\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1234, in _get_data\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  success, data = self._try_get_data()\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1095, in _try_get_data\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n",
      "1ub1bj9v1l-algo-1-ntx1p  |  RuntimeError: DataLoader worker (pid(s) 76) exited unexpectedly\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  | Command \"/opt/conda/bin/python3.8 train.py --batch_size 4 --epochs 1 --samples_per_epoch 500\"\n",
      "1ub1bj9v1l-algo-1-ntx1p  | 2023-08-02 18:16:40,725 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:creating /tmp/tmpsce_mp_k/artifacts/output/data\n",
      "INFO:root:copying /tmp/tmpsce_mp_k/algo-1-ntx1p/output/failure -> /tmp/tmpsce_mp_k/artifacts/output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ub1bj9v1l-algo-1-ntx1p exited with code 1\n",
      "Aborting on container exit...\n",
      "Container 1ub1bj9v1l-algo-1-ntx1p  Stopping\n",
      "Container 1ub1bj9v1l-algo-1-ntx1p  Stopped\n",
      "time=\"2023-08-02T22:16:50Z\" level=error msg=1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/tmp/tmpsce_mp_k/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/image.py:255\u001b[0m, in \u001b[0;36m_SageMakerContainer.train\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[43m_stream_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# _stream_output() doesn't have the command line. We will handle the exception\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# which contains the exit code and append the command line to it.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/image.py:928\u001b[0m, in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exit_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess exited with code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m exit_code)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exit_code\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://fastvision.ai/segmented_data/LUNA16_segmented_2mm_test/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:300\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1260\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1259\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2206\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \n\u001b[1;32m   2183\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;124;03m    all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[0;32m-> 2206\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:888\u001b[0m, in \u001b[0;36mSession.train\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    885\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m--> 888\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:5417\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5401\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5402\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5405\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5406\u001b[0m ):\n\u001b[1;32m   5407\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5408\u001b[0m \n\u001b[1;32m   5409\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5415\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5416\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:886\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    884\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training-job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m    885\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 886\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/local_session.py:200\u001b[0m, in \u001b[0;36mLocalSagemakerClient.create_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, Environment, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    199\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training job\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[43mtraining_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mInputDataConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOutputDataConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEnvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingJobName\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m LocalSagemakerClient\u001b[38;5;241m.\u001b[39m_training_jobs[TrainingJobName] \u001b[38;5;241m=\u001b[39m training_job\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/entities.py:243\u001b[0m, in \u001b[0;36m_LocalTrainingJob.start\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_TRAINING\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment \u001b[38;5;241m=\u001b[39m environment\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_artifacts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_data_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_COMPLETED\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/local/image.py:260\u001b[0m, in \u001b[0;36m_SageMakerContainer.train\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# _stream_output() doesn't have the command line. We will handle the exception\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# which contains the exit code and append the command line to it.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (compose_command, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     artifacts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_artifacts(compose_data, output_data_config, job_name)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/tmp/tmpsce_mp_k/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": \"s3://fastvision.ai/segmented_data/LUNA16_segmented_2mm_test/\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Create endpoint\n",
    "After training, we use the `PyTorch` estimator object to build and deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "You can use the test images to evalute the endpoint. The accuracy of the model depends on how many it is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = predictor.predict(data))\n",
    "# print(\"Raw prediction result:\")\n",
    "# print(response)\n",
    "# print()\n",
    "\n",
    "# labeled_predictions = list(zip(range(10), response[0]))\n",
    "# print(\"Labeled predictions: \")\n",
    "# print(labeled_predictions)\n",
    "# print()\n",
    "\n",
    "# labeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\n",
    "# print(\"Most likely answer: {}\".format(labeled_predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name=predictor.endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
