{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "instance_type = 'ml.g4dn.2xlarge'\n",
    "instance_count = 4\n",
    "\n",
    "model_dir = \"/opt/ml/model\"\n",
    "\n",
    "distributions={\n",
    "    \"smdistributed\": {\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\":True,\n",
    "            \"parameters\": {\n",
    "                \"partitions\": 4,\n",
    "                \"microbatches\": 4,\n",
    "                \"placement_strategy\": \"spread\",\n",
    "                \"pipeline\": \"interleaved\",\n",
    "                \"optimize\": \"speed\",\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    \"mpi\": {\n",
    "        \"enabled\": True,\n",
    "        \"processes_per_host\": 8,\n",
    "    }\n",
    "}\n",
    "\n",
    "hyperparameters = {'epochs': 100, 'batch-size': 8}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "                       source_dir='./code',\n",
    "                       entry_point='train.py',\n",
    "                       base_job_name='3dcnn-tf',\n",
    "                       role=role,\n",
    "                       framework_version='2.2',\n",
    "                       py_version='py37',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       instance_count=instance_count,\n",
    "                       instance_type=instance_type,\n",
    "                       model_dir=model_dir,\n",
    "                       distribution=distributions,\n",
    "                       session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: 3dcnn-tf-2023-08-06-18-12-00-894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-08-06 18:12:01 Starting - Starting the training job...\n",
      "2023-08-06 18:12:16 Starting - Preparing the instances for training......\n",
      "2023-08-06 18:13:31 Downloading - Downloading input data......\n",
      "2023-08-06 18:14:11 Training - Downloading the training image......\n",
      "2023-08-06 18:15:12 Training - Training image download completed. Training in progress..\u001b[34m2023-08-06 18:15:35.646605: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:35.646771: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:35.672105: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:36,885 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:37,091 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting SimpleITK\n",
      "  Downloading SimpleITK-2.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (1.18.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: SimpleITK\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:31.741665: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:31.741827: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:31.766992: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:32,959 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:33,164 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[32m/usr/local/bin/python3.7 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[32mCollecting SimpleITK\n",
      "  Downloading SimpleITK-2.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.0.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (1.18.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[32mInstalling collected packages: SimpleITK\u001b[0m\n",
      "\u001b[32mSuccessfully installed SimpleITK-2.2.1\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:33.075915: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:33.076074: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:33.100924: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:34,276 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:34,473 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[36m/usr/local/bin/python3.7 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[36mCollecting SimpleITK\n",
      "  Downloading SimpleITK-2.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.0.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (1.18.5)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[36mInstalling collected packages: SimpleITK\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:32.599083: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:32.599226: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:32.624211: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:33,795 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:34,008 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting SimpleITK\n",
      "  Downloading SimpleITK-2.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (1.18.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: SimpleITK\u001b[0m\n",
      "\u001b[32mWARNING: You are using pip version 21.0.1; however, version 23.2.1 is available.\u001b[0m\n",
      "\u001b[32mYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:36,967 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:36,967 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:36,969 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:36,969 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.88.88\u001b[0m\n",
      "\u001b[34mSuccessfully installed SimpleITK-2.2.1\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 21.0.1; however, version 23.2.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:37,874 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:37,874 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:37,880 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:37,881 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:37,881 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.108.243\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:37,975 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:38,069 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:38,069 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:38,069 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:38,069 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:38,077 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[36mSuccessfully installed SimpleITK-2.2.1\u001b[0m\n",
      "\u001b[36mWARNING: You are using pip version 21.0.1; however, version 23.2.1 is available.\u001b[0m\n",
      "\u001b[36mYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,241 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,242 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,247 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,341 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,341 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,341 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,341 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:38,349 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:38,883 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:38,883 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.108.243\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:39,885 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:39,885 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.108.243\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:40,886 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:40,886 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.108.243\u001b[0m\n",
      "\u001b[35mSuccessfully installed SimpleITK-2.2.1\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 21.0.1; however, version 23.2.1 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:40,895 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:40,896 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:40,901 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:40,997 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:40,997 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:40,997 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:40,997 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:41,006 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:41,892 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:41,986 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:41,986 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:41,987 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:41,998 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,097 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,097 sagemaker-training-toolkit INFO     Can connect to host algo-3\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,097 sagemaker-training-toolkit INFO     Worker algo-3 available for communication\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,103 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,153 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,153 sagemaker-training-toolkit INFO     Can connect to host algo-4\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,154 sagemaker-training-toolkit INFO     Worker algo-4 available for communication\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,154 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4'] Hosts: ['algo-1:8', 'algo-2:8', 'algo-3:8', 'algo-4:8'] process_per_hosts: 8 num_processes: 32\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,155 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:42,189 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 8,\n",
      "        \"epochs\": 100,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"mp_parameters\": {\n",
      "            \"partitions\": 4,\n",
      "            \"microbatches\": 4,\n",
      "            \"placement_strategy\": \"spread\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\"\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"3dcnn-tf-2023-08-06-18-12-00-894\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-049084878542/3dcnn-tf-2023-08-06-18-12-00-894/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":8,\"epochs\":100,\"model_dir\":\"/opt/ml/model\",\"mp_parameters\":{\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.g4dn.2xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-4\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-049084878542/3dcnn-tf-2023-08-06-18-12-00-894/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.g4dn.2xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"hyperparameters\":{\"batch-size\":8,\"epochs\":100,\"model_dir\":\"/opt/ml/model\",\"mp_parameters\":{\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"3dcnn-tf-2023-08-06-18-12-00-894\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-049084878542/3dcnn-tf-2023-08-06-18-12-00-894/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-4\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"8\",\"--epochs\",\"100\",\"--model_dir\",\"/opt/ml/model\",\"--mp_parameters\",\"microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8,algo-3:8,algo-4:8 -np 32 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_BATCH-SIZE -x SM_HP_EPOCHS -x SM_HP_MODEL_DIR -x SM_HP_MP_PARAMETERS -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py train.py --batch-size 8 --epochs 100 --model_dir /opt/ml/model --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\n",
      " Data for JOB [14922,1] offset 0 Total slots allocated 32\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: ip-10-0-88-88#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 15 Bound: N/A\n",
      " Data for node: algo-3#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 16 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 17 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 18 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 19 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 20 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 21 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 22 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 23 Bound: N/A\n",
      " Data for node: algo-4#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 24 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 25 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 26 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 27 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 28 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 29 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 30 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 31 Bound: N/A\n",
      " =============================================================\n",
      " Data for JOB [14922,1] offset 0 Total slots allocated 32\n",
      " Data for JOB [14922,1] offset 0 Total slots allocated 32\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: ip-10-0-88-88#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 15 Bound: N/A\n",
      " Data for node: algo-3#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 16 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 17 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 18 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 19 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 20 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 21 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 22 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 23 Bound: N/A\n",
      " Data for node: algo-4#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 24 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 25 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 26 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 27 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 28 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 29 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 30 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 31 Bound: N/A\n",
      " =============================================================\n",
      " Data for JOB [14922,1] offset 0 Total slots allocated 32\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: ip-10-0-88-88#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 15 Bound: N/A\n",
      " Data for node: algo-3#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 16 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 17 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 18 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 19 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 20 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 21 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 22 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 23 Bound: N/A\n",
      " Data for node: algo-4#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 24 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 25 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 26 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 27 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 28 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 29 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 30 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 31 Bound: N/A\n",
      " =============================================================\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: ip-10-0-88-88#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 15 Bound: N/A\n",
      " Data for node: algo-3#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 16 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 17 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 18 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 19 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 20 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 21 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 22 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 23 Bound: N/A\n",
      " Data for node: algo-4#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 24 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 25 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 26 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 27 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 28 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 29 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 30 Bound: N/A\n",
      " #011Process OMPI jobid: [14922,1] App: 0 Process rank: 31 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:43,012 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:43,012 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:43,012 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:43,087 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:43,087 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:43,087 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:43,359 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:43,359 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:43,360 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=57, name='orted', status='sleeping', started='18:15:42')]\u001b[0m\n",
      "\u001b[32m2023-08-06 18:15:45,573 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[36m2023-08-06 18:15:45,560 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-4,10.0.119.230' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-3,10.0.94.87' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.108.243' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2023-08-06 18:15:43.071332: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2023-08-06 18:15:43.071593: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:2023-08-06 18:15:43.081452: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:2023-08-06 18:15:43.081430: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:2023-08-06 18:15:43.081431: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:2023-08-06 18:15:43.081452: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:2023-08-06 18:15:43.081483: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:2023-08-06 18:15:43.081608: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:2023-08-06 18:15:43.081608: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:2023-08-06 18:15:43.081611: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:2023-08-06 18:15:43.081765: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:2023-08-06 18:15:43.081949: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:2023-08-06 18:15:43.082564: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:2023-08-06 18:15:43.082749: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:2023-08-06 18:15:43.083683: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:2023-08-06 18:15:43.083866: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2023-08-06 18:15:43.084722: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2023-08-06 18:15:43.084874: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2023-08-06 18:15:43.085426: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2023-08-06 18:15:43.085651: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2023-08-06 18:15:43.085681: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2023-08-06 18:15:43.085886: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2023-08-06 18:15:43.085915: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2023-08-06 18:15:43.085915: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2023-08-06 18:15:43.086058: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2023-08-06 18:15:43.086078: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2023-08-06 18:15:43.086105: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2023-08-06 18:15:43.086209: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2023-08-06 18:15:43.088625: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2023-08-06 18:15:43.088726: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:2023-08-06 18:15:43.089244: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:2023-08-06 18:15:43.089419: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:2023-08-06 18:15:43.091132: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:2023-08-06 18:15:43.091322: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2023-08-06 18:15:43.090916: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2023-08-06 18:15:43.090918: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2023-08-06 18:15:43.090912: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2023-08-06 18:15:43.090909: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2023-08-06 18:15:43.090911: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2023-08-06 18:15:43.090909: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2023-08-06 18:15:43.090909: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2023-08-06 18:15:43.091090: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2023-08-06 18:15:43.091215: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2023-08-06 18:15:43.091453: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:2023-08-06 18:15:43.092442: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:2023-08-06 18:15:43.092551: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:2023-08-06 18:15:43.092657: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:2023-08-06 18:15:43.092745: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2023-08-06 18:15:43.093030: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2023-08-06 18:15:43.093237: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:2023-08-06 18:15:43.095613: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:2023-08-06 18:15:43.095754: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:2023-08-06 18:15:43.097386: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:2023-08-06 18:15:43.097464: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:2023-08-06 18:15:43.097544: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:2023-08-06 18:15:43.097666: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:2023-08-06 18:15:43.099710: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:2023-08-06 18:15:43.099859: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:2023-08-06 18:15:43.101022: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:2023-08-06 18:15:43.101136: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2023-08-06 18:15:43.111325: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:2023-08-06 18:15:43.122235: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:2023-08-06 18:15:43.122235: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:2023-08-06 18:15:43.122245: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:2023-08-06 18:15:43.122230: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:2023-08-06 18:15:43.122376: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:2023-08-06 18:15:43.122387: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-08-06 18:15:45,597 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:2023-08-06 18:15:43.124669: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2023-08-06 18:15:43.125321: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2023-08-06 18:15:43.126122: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2023-08-06 18:15:43.126152: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2023-08-06 18:15:43.127016: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2023-08-06 18:15:43.127957: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2023-08-06 18:15:43.127954: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2023-08-06 18:15:43.128520: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:2023-08-06 18:15:43.129413: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:2023-08-06 18:15:43.130729: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:2023-08-06 18:15:43.131562: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2023-08-06 18:15:43.131564: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2023-08-06 18:15:43.131563: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2023-08-06 18:15:43.131768: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2023-08-06 18:15:43.131779: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2023-08-06 18:15:43.131837: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2023-08-06 18:15:43.132073: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2023-08-06 18:15:43.132456: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1:8,algo-2:8,algo-3:8,algo-4:8 -np 32 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_BATCH-SIZE -x SM_HP_EPOCHS -x SM_HP_MODEL_DIR -x SM_HP_MP_PARAMETERS -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py train.py --batch-size 8 --epochs 100 --model_dir /opt/ml/model --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\"\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-4,10.0.119.230' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-3,10.0.94.87' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.108.243' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2023-08-06 18:15:43.071332: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2023-08-06 18:15:43.071593: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:2023-08-06 18:15:43.081452: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2023-08-06 18:15:43.133236: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:2023-08-06 18:15:43.081430: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:2023-08-06 18:15:43.081431: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:2023-08-06 18:15:43.081452: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:2023-08-06 18:15:43.081483: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:2023-08-06 18:15:43.081608: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:2023-08-06 18:15:43.081608: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:2023-08-06 18:15:43.081611: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:2023-08-06 18:15:43.081765: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:2023-08-06 18:15:43.081949: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:2023-08-06 18:15:43.135499: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:2023-08-06 18:15:43.136277: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:2023-08-06 18:15:43.136678: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:2023-08-06 18:15:43.137139: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:2023-08-06 18:15:43.140194: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:2023-08-06 18:15:43.140784: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:2023-08-06 18:15:43.082564: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:2023-08-06 18:15:43.082749: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:2023-08-06 18:15:43.083683: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:2023-08-06 18:15:43.083866: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2023-08-06 18:15:43.084722: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2023-08-06 18:15:43.084874: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2023-08-06 18:15:43.085426: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2023-08-06 18:15:43.085651: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2023-08-06 18:15:43.085681: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2023-08-06 18:15:43.085886: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2023-08-06 18:15:43.085915: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2023-08-06 18:15:43.085915: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2023-08-06 18:15:43.086058: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2023-08-06 18:15:43.086078: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2023-08-06 18:15:43.086105: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2023-08-06 18:15:43.086209: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2023-08-06 18:15:43.088625: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2023-08-06 18:15:43.088726: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:2023-08-06 18:15:43.089244: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:2023-08-06 18:15:43.089419: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:2023-08-06 18:15:43.091132: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:2023-08-06 18:15:43.091322: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2023-08-06 18:15:43.090916: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2023-08-06 18:15:43.090918: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2023-08-06 18:15:43.090912: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2023-08-06 18:15:43.090909: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2023-08-06 18:15:43.090911: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2023-08-06 18:15:43.090909: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2023-08-06 18:15:43.090909: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2023-08-06 18:15:43.091091: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2023-08-06 18:15:43.091090: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2023-08-06 18:15:43.091215: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2023-08-06 18:15:43.091453: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:2023-08-06 18:15:43.092442: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:2023-08-06 18:15:43.092551: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:2023-08-06 18:15:43.092657: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:2023-08-06 18:15:43.092745: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2023-08-06 18:15:43.093030: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2023-08-06 18:15:43.093237: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:2023-08-06 18:15:43.095613: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:2023-08-06 18:15:43.095754: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:2023-08-06 18:15:43.097386: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:2023-08-06 18:15:43.097464: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:2023-08-06 18:15:43.097544: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:2023-08-06 18:15:43.097666: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:2023-08-06 18:15:43.099710: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:2023-08-06 18:15:43.099859: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:2023-08-06 18:15:43.101022: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:2023-08-06 18:15:43.101136: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2023-08-06 18:15:43.111325: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:2023-08-06 18:15:43.122235: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:2023-08-06 18:15:43.122235: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:2023-08-06 18:15:43.122245: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:2023-08-06 18:15:43.122230: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:2023-08-06 18:15:43.122376: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:2023-08-06 18:15:43.122387: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:2023-08-06 18:15:43.124669: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2023-08-06 18:15:43.125321: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2023-08-06 18:15:43.126122: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2023-08-06 18:15:43.126152: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2023-08-06 18:15:43.127016: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2023-08-06 18:15:43.127957: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2023-08-06 18:15:43.127954: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2023-08-06 18:15:43.128520: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:2023-08-06 18:15:43.129413: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:2023-08-06 18:15:43.130729: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:2023-08-06 18:15:43.131562: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2023-08-06 18:15:43.131564: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2023-08-06 18:15:43.131563: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2023-08-06 18:15:43.131768: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2023-08-06 18:15:43.131779: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2023-08-06 18:15:43.131837: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2023-08-06 18:15:43.132073: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2023-08-06 18:15:43.132456: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2023-08-06 18:15:43.133236: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:2023-08-06 18:15:43.135499: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:2023-08-06 18:15:43.136277: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:2023-08-06 18:15:43.136678: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:2023-08-06 18:15:43.137139: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:2023-08-06 18:15:43.140194: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:2023-08-06 18:15:43.140784: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,16]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,30]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,27]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,23]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,24]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,17]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,29]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,26]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,19]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,20]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,31]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,25]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,18]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,22]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,21]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[14922,1],3]\n",
      "  Exit code:    2\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,28]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:usage: train.py [-h] [--hosts HOSTS] [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--data-dir DATA_DIR] --model_dir MODEL_DIR\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--model_output_dir MODEL_OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--output_data_dir OUTPUT_DATA_DIR] [--output-dir OUTPUT_DIR]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--tensorboard-dir TENSORBOARD_DIR]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--weight-decay WEIGHT_DECAY] [--learning-rate LEARNING_RATE]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--data-config DATA_CONFIG] [--fw-params FW_PARAMS]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:                [--optimizer OPTIMIZER] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:train.py: error: unrecognized arguments: --mp_parameters microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[14922,1],3]\n",
      "  Exit code:    2\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-08-06 18:15:45,614 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\n",
      "2023-08-06 18:16:04 Uploading - Uploading generated training model\n",
      "2023-08-06 18:16:04 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job 3dcnn-tf-2023-08-06-18-12-00-894: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8,algo-2:8,algo-3:8,algo-4:8 -np 32 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://fastvision.ai/segmented_data/LUNA16_segmented_4mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:300\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1263\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2425\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2425\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:4808\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4788\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4789\u001b[0m \n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4808\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:6677\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6674\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6677\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   6679\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:6730\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6726\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6727\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6728\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6729\u001b[0m     )\n\u001b[0;32m-> 6730\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6731\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6732\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6733\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6734\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job 3dcnn-tf-2023-08-06-18-12-00-894: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8,algo-2:8,algo-3:8,algo-4:8 -np 32 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRA"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": \"s3://fastvision.ai/segmented_data/LUNA16_segmented_4mm\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
